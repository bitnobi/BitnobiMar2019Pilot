A Bitnobi Workflow starts with one or more `Datasource` elements, performs an optional sequence of `Operations` and ends with a `Result` element. 

When you drag a workflow element onto the canvas and then click on it, the properties are displayed in the right-hand pane. After you set the properties, clicking on the "Apply" button will populate a preview of a few rows of output generated by this workflow element and display it in the "Properties" pane at the bottom of the screen. The "Apply" operation also relays the column names to the next element in the workflow, so it is crucial to press "Apply" on each element while creating a workflow.

Below is a summary of the element types. Clicking on the element name will jump to a detailed description.

|Category|Element Name|Description|
|--------|------------|----|
|**Datasources**| | provides ways of getting data|
| |[Audit Log](#audit-log)| records timestamped Bitnobi events |
| |[Datasource](#datasource) |uses result set from the last run of the specified workflow. |
| |[ExternalDS](#externalds) |access result set from a remote Bitnobi server. |
| |[Importer](#importer) | allows user to upload a .csv or .json file to use as a datasource |
| |[Jupyter](#jupyter) | uses output of a Jupyter Notebooks session as a datasource. |
| |[SQL](#sql)| allows specifying an external SQL database and query to use as a datasource|
| |[Workflow](#workflow) |runs the specified workflow and uses its result set |
| **Operations**|  | elements in a workflow that modify data|
| |[Alter](#alter)| allows disabling fields, renaming fields and changing field datatype| |
| |[Custom Query](#custom-query)| allows user to create a Javascript function to modify the row contents or create new columns. |
| |[Data Mover](#data-mover) |moves temporary data from a remote Bitnobi server to the current server. |
| |[Group](#group)| |
| |[Join](#join)| combines two data flows based on the common columns in the two flows (like a SQL inner join)|
| |[MapReduce](#mapreduce)| a data processing paradigm for condensing large volumes of data into useful aggregated results. |
| |[Python](#python)| allows the user to create a Python function to modify the row contents or create new rows. |
| |[Select](#select)| allows filtering out columns and/or filtering rows based on matching conditions (where clause)|
| |[Union](#union) |concatenates two data sets with the same schema (like SQL Union). |
|**Result**|| |
| |[Result](#result)|persists the result of the workflow and allows setting policies to control sharing. |

Below is a description of each of the element types.
*** 
# Datasources

## Audit Log
* only accessible to an admin user
* records timestamped events and the user requesting it for:
  * user signin/signup/signout or modification,
  * policy, report and workflow creation, modification deletion
  * workflow start, end,
  * datasource access

## Datasource
* takes the result of a workflow execution as input to the current workflow
* the properties pane displays a drop-down list of workflow names with result sets available to you. 
Only those results where the a policy matches your user attributes will appear in this list.

## ExternalDS
* access a result set from a remote Bitnobi server
* in the properties of this element, there is an 'Open Input Setting' button that pops up a dialog box to allow selecting a remote bitnobi server. You can then press the "Search" button to get a list of available result sets on that server in the "Available Data" pane, or enter a query string to match specific names. Pressing the "->" button beside a result set will select it as an input, then press "Save".
* note that your user attributes are sent to the remote bitnobi server these are applied against its the policies on that server to determine which result sets are accessible to you. Only the result sets accessible to you will appear in the "Available Data" pane.
* when you press the "Apply" button for this element, it will fetch a preview of the data from the remote bitnobi server.
* when another operations element is linked to an ExternalDS, that element will be executed on the remote bitnobi server hosting the result set and any intermediate data will remain on the remote server.
* entries in the "Select Remote Source" list must be defined by a Bitnobi administrator using the "Network Management > Bitnobi Servers" page.

## Importer
* allows uploading an external data file to Bitnobi to use a a datasource.
* supports comma separated values (.csv) files and JSON (.json) format files.

## Jupyter
* if you use the bundled Jupyter Notebooks application to push data to Bitnobi, the available data sets will appear in a drop down listbox in the properties pane.

## SQL
* allows connecting to and querying an external MySQL database to use as a datasource

## Workflow
* executes the named workflow and uses its result set as a datasource.
* can define a "chain" of workflows where the output of one is used an an input to another. This chain will be executed sequentially.
* the properties pane displays a drop-down list of workflow names with result sets available to you. Only those results where the a policy matches your user attributes will appear in this list.
* while editing workflow, "Apply" for this element will not execute the underlying workflow but rather depends on previous workflow run to provide preview data and schema. 

*** 
# Operations
## Alter
* allows changing the names of columns and/or disabling specific columns in the data flow
* allows changing column datatypes. Some of the datasources may import numeric columns as a "string" datatype which can cause unexpected behaviour in a Select Where clause. 

## Custom Query
* allows writing a Javascript function to modify the row contents or creating new columns. This function is executed on a per-row basis.
* in the properties of this element, there is an 'edit' button that pops up a Javascript editor
* here is an example of code that creates a new column "Full Name" from existing column values "first" and "last":
```
    row['Full Name'] = row['first'] + ' '+ row['last'];
    return row;
```
* Javascript has fairly powerful [string manipulation functions](https://www.digitalocean.com/community/tutorials/how-to-index-split-and-manipulate-strings-in-javascript) as well as [math operators](https://www.digitalocean.com/community/tutorials/how-to-do-math-in-javascript-with-operators).
* if a column contains free-form text, string matching can be used to count the occurrence of keywords and generate sentiment-analysis type data.


## Data Mover
* moves temporary data from a remote Bitnobi server to your local Bitnobi server.
* a chain of workflow elements attached to an ExternalDS are all executed on the remote Bitnobi server. Once a Data Mover is added to the chain, the data flow after this point will execute on the local Bitnobi server.
* typically a Data Mover element is placed before a Join or Union element to allow merging data across servers. 

## Group

## Join
* takes two data flow inputs and generates an output that is an "inner join" of the two data sets.
* in the properties pane, select the column from each data input that will be used as common columns for the join.
* can also be used to perform a "lookup" function. For example lets say that input 1 has a list of names as well as a "status-id" column containing either 0 or 1. And lets say that input 2 has two columns:
```
status-id,status-text
0, inactive
1, active
```
Then the join operation choosing "status-id" as the common column will produce an output like input 1 but with an extra column "status-text" containing "inactive" wherever "status-id" is 0 and "active" wherever "status-id" is 1.
* note that both inputs to a Join element must be on the same Bitnobi server. A Data Mover is required to move the data flow from an ExternalDS to the local server prior to a Join.

## MapReduce
* in the properties of this element, there is an 'edit' button that pops up the MapReduce editor. There are two main editing panes - for the Map function and for the Reduce function. Both execute Javascript code.
* the Map function is invoked once per row and typically ends by emitting a (key, value) pair using the syntax:
```
emit(this.data['columnName'], value);
```
where `columnName` is the name of a column in the dataset and `value` is some calculated value.
* The Reduce operations are executed after all the Map operations are complete. The Reduce function is invoked once per emitted key and is passed the variables `key` and `values`. The `values` contains an array of values that were emitted for one particular key. An aggregation function is typically done on the values and then returned.
* The MapReduce operation returns a data set containing 2 columns - an "id" column that contains the key values and a "value" column that is the result of the Reduce operation on the value. For example if we have an input dataset that has a "first name" column and use the Map function:
```
    emit(this.data['first name'], 1);
```
followed by a Reduce function like:
```
    return Array.sum(values);
```
This will result in a data set with the "id" column listing a first name and the "value" column containing the number of times that first name was in the input dataset.

## Python
* allows writing a Python function to modify the data flow. The NumPy library (math functions for large, multi-dimensional arrays and matrices), the SciPy library (optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing) as well as TensorFlow library (for building Deep Learning models) are all supported in Bitnobi.
* in the properties of this element, there is an 'edit' button that pops up a Python editor
* pressing the "Sample Template" > "Add" button will insert a sample program:
```
 # gets and displays preview input from canvas
bitnobiInput = io.getInput()
for row in bitnobiInput:
    print(row)

# reset input cursor, use only if needed to loop through the input another time
io.resetInput()

# write to output
io.writeOutput(['name','id'])
io.writeOutput(['justin','1'])
io.writeOutput(['john','2'])
```
* Bitnobi provides the built-in functions `io.getInput()` for accessing the input data stream one row at a time and `io.writeOutput()` to write out one row at a time.
* pressing the "Execute" button in the Python editor will execute the program with preview data, display the print statements in "Console" pane and the workflow output in the "Output Options" pane. The console output is for debugging only. Only the results generated by the `io.writeOutput()` statements will appear in the output data stream when the workflow is executed.
* pressing the "Save" button saves the code and exits the Python editor. Pressing "Apply" in the Python properties will execute the Python code on the preview data and populate the Preview pane.

## Select
* allows filtering out columns in the data stream.
* allows restricting which rows are passed along by creating "where" clauses that match conditions on specific columns. Note that for numeric columns, it may be necessary to use the "Alter" block to cast the datatype as "Integer" or "Float" to get proper numeric comparisons (e.g. selecting rows where id < 50). For string datatype the comparison is in dictionary (lexicographic) order. String comparisons can also use the "like" operation which matches substrings.

## Union
* concatenates two data sets with the same schema (like SQL Union).
* It does not remove duplicate rows between the two input data sets.

*** 
# Result
## Result
* Each `Result` must have one or more `Policies` attached to it and the result is shared with users that match one of the policies.


